#+title: Interesting Candidates
Baseline parameters
#+begin_src bash
#L1D/L1I
  localparam CVA6ConfigIcacheByteSize = 4096;
  localparam CVA6ConfigIcacheSetAssoc = 4;
  localparam CVA6ConfigIcacheLineWidth = 128;
  localparam CVA6ConfigDcacheByteSize = 4096;
  localparam CVA6ConfigDcacheSetAssoc = 4;
  localparam CVA6ConfigDcacheLineWidth = 128;
# BTB BHT RAS
  localparam CVA6ConfigRASDepth = 2;
  localparam CVA6ConfigBTBEntries = 16;
  localparam CVA6ConfigBHTEntries = 16;
# TLB (not increase too much)
  InstrTlbEntries: int'(4),
  DataTlbEntries: int'(4),
#+end_src

* Parameter sweep

|--------------------+------------------------------+--------------------------------+-------------------------------------------------------------------|
| *Parameter Changed*  | *Sweep Values (Range)*         | *Constants (Locked to Baseline)* | *Goal*                                                              |
|--------------------+------------------------------+--------------------------------+-------------------------------------------------------------------|
| Cache Size (I & D) | 2 KB, 8 KB, 16 KB, 32 KB     | Assoc=4, BTB=16, BHT=16        | Measure impact of capacity on cache misses.                       |
| Associativity      | 1-Way (Direct), 2-Way, 8-Way | Size=4KB, BTB=16, BHT=16       | Measure impact of conflict misses vs. logic complexity.           |
| BTB Entries        | 8, 32, 64                    | Size=4KB, Assoc=4, BHT=16      | Measure impact on branch target prediction (loops/jumps).         |
| BHT Entries        | 32, 128, 256                 | Size=4KB, Assoc=4, BTB=16      | Measure impact on conditional branch direction (taken/not taken). |
|--------------------+------------------------------+--------------------------------+-------------------------------------------------------------------|

* Performed experiment data
** Baseline
#+name: baseline
|-----------------------------------------------------------------------+-----------------+-------------------------+-------------------------------|
| *Bitstream*                                                             | *Characteristics* | *NPB results file*        | *Unixbench results file*        |
|-----------------------------------------------------------------------+-----------------+-------------------------+-------------------------------|
| ariane_IC4k_4w_128L_DC4k_4w_128L_BTB16_BHT16_RAS2_ITLB4_DTLB4.bit.bin | baseline        | NPB_19700101_001944.txt | unixbench_19700101_004021.txt |
|-----------------------------------------------------------------------+-----------------+-------------------------+-------------------------------|

#+begin_src jupyter-python :var baseline=baseline :results output drawer :exports results
import os
import re
import pandas as pd

# 1. Setup Paths
NPB_DIR = "/home/han4n/cva6_experiments/runs/NPB"
UNIX_DIR = "/home/han4n/cva6_experiments/runs/UnixBench"
OUTPUT_DIR = "/home/han4n/cva6_experiments/results"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "baseline_results.csv")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# 2. Regex Patterns
re_begin_npb = re.compile(r">>>BEGIN_TEST\|NPB_(\w+)_")
re_begin_unix = re.compile(r">>>BEGIN_TEST\|(\w+)")
re_end = re.compile(r">>>END_TEST")

# Metrics
re_mops = re.compile(r"\s*Mop/s total\s*=\s*([\d\.]+)")
re_unix_count = re.compile(r"COUNT\|([\d\.]+)\|(\d+)\|(\w+)")
re_papi_inst = re.compile(r"\[PAPI\] Instructions:\s*(\d+)")
re_papi_cyc = re.compile(r"\[PAPI\] Cycles:\s*(\d+)")
re_papi_ipc = re.compile(r"\[PAPI\] IPC:\s*([\d\.]+)")

def parse_logs(file_path, suite_type):
    if not os.path.exists(file_path):
        print(f"Warning: File not found: {file_path}")
        return []

    results = []
    with open(file_path, 'r') as f:
        lines = f.readlines()

    current_test = None
    metrics = {}
    captured_score = False

    for line in lines:
        test_name = None
        if suite_type == "NPB":
            m = re_begin_npb.match(line)
            if m: test_name = m.group(1)
        elif suite_type == "UnixBench":
            m = re_begin_unix.match(line)
            if m: test_name = m.group(1)

        if test_name:
            current_test = test_name
            metrics = {}
            captured_score = False
            continue

        if re_end.match(line):
            if current_test and 'score' in metrics:
                metrics['name'] = current_test
                results.append(metrics)
            current_test = None
            continue

        if current_test:
            # Capture Score (First Valid Occurrence)
            if not captured_score:
                if suite_type == "NPB":
                    m = re_mops.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = "Mop/s"
                        captured_score = True
                elif suite_type == "UnixBench":
                    m = re_unix_count.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = m.group(3)
                        captured_score = True

            # PAPI Stats
            m_inst = re_papi_inst.match(line)
            if m_inst: metrics['instructions'] = int(m_inst.group(1))
            m_cyc = re_papi_cyc.match(line)
            if m_cyc: metrics['cycles'] = int(m_cyc.group(1))
            m_ipc = re_papi_ipc.match(line)
            if m_ipc: metrics['ipc'] = float(m_ipc.group(1))

    return results

# 3. Process Baseline Data
all_data = []

# Handle header row if present
input_rows = baseline
if len(input_rows) > 0 and ("Bitstream" in input_rows[0][0] or "*Bitstream*" in input_rows[0][0]):
    input_rows = input_rows[1:]

for row in input_rows:
    desc = row[1]
    npb_file = row[2]
    unix_file = row[3]

    # Use "baseline" as the config tag (or whatever is in the Characteristics column)
    config_tag = desc

    # Parse NPB
    for res in parse_logs(os.path.join(NPB_DIR, npb_file), "NPB"):
        res['config'] = config_tag
        res['suite'] = "NPB"
        all_data.append(res)

    # Parse UnixBench
    for res in parse_logs(os.path.join(UNIX_DIR, unix_file), "UnixBench"):
        res['config'] = config_tag
        res['suite'] = "UnixBench"
        all_data.append(res)

# 4. Save to CSV
if all_data:
    df = pd.DataFrame(all_data)
    cols = ['config', 'suite', 'name', 'score', 'units', 'instructions', 'cycles', 'ipc']
    # Select available columns
    df = df[[c for c in cols if c in df.columns]]

    df.to_csv(OUTPUT_FILE, index=False)
    print(f"Success! Baseline data saved to: {OUTPUT_FILE}")
    print(f"Total rows: {len(df)}")
else:
    print("Error: No data extracted. Check file paths and regex.")
#+end_src

#+RESULTS:
:results:
#+begin_exampleSuccess! Baseline data saved to: /home/han4n/cva6_experiments/results/baseline_results.csv
Total rows: 20
#+end_example
:end:

** Cache sweep
#+name: cache_sweep
|-------------------------------------------------------------------------+-----------------------------------+-------------------------+-------------------------------|
| *Bitstream*                                                               | *Characteristics*                   | *NPB results file*        | *Unixbench results file*        |
|-------------------------------------------------------------------------+-----------------------------------+-------------------------+-------------------------------|
| ariane_IC2k_4w_128L_DC2k_4w_128L_BTB16_BHT16_RAS2_ITLB4_DTLB4.bit.bin   | Instruction and Data caches = 2k  | NPB_19700101_000455.txt | unixbench_19700101_002109.txt |
| ariane_IC8k_4w_128L_DC8k_4w_128L_BTB16_BHT16_RAS2_ITLB4_DTLB4.bit.bin   | Instruction and Data caches = 8k  | NPB_19700101_000203.txt | unixbench_19700101_001851.txt |
| ariane_IC16k_4w_128L_DC16k_4w_128L_BTB16_BHT16_RAS2_ITLB4_DTLB4.bit.bin | Instruction and Data caches = 16k | NPB_19700101_000205.txt | unixbench_19700101_001810.txt |
| ariane_IC32k_4w_128L_DC32k_4w_128L_BTB16_BHT16_RAS2_ITLB4_DTLB4.bit.bin | Instruction and Data caches = 32k | NPB_19700101_000334.txt | unixbench_19700101_002247.txt |
|-------------------------------------------------------------------------+-----------------------------------+-------------------------+-------------------------------|

#+begin_src jupyter-python :var cache_sweep=cache_sweep :results output drawer :exports results

import os
import re
import pandas as pd

# 1. Setup Paths
NPB_DIR = "/home/han4n/cva6_experiments/runs/NPB"
UNIX_DIR = "/home/han4n/cva6_experiments/runs/UnixBench"
OUTPUT_DIR = "/home/han4n/cva6_experiments/results"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "cache_sweep_benchmark_results.csv")

# Ensure output directory exists
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 2. Regex Patterns
re_begin_npb = re.compile(r">>>BEGIN_TEST\|NPB_(\w+)_")
re_begin_unix = re.compile(r">>>BEGIN_TEST\|(\w+)")
re_end = re.compile(r">>>END_TEST")

# Metrics
re_mops = re.compile(r"\s*Mop/s total\s*=\s*([\d\.]+)")
re_unix_count = re.compile(r"COUNT\|([\d\.]+)\|(\d+)\|(\w+)")
re_papi_inst = re.compile(r"\[PAPI\] Instructions:\s*(\d+)")
re_papi_cyc = re.compile(r"\[PAPI\] Cycles:\s*(\d+)")
re_papi_ipc = re.compile(r"\[PAPI\] IPC:\s*([\d\.]+)")

def parse_logs(file_path, suite_type):
    """Parses a single log file and returns a list of result dicts."""
    if not os.path.exists(file_path):
        return []

    results = []
    with open(file_path, 'r') as f:
        lines = f.readlines()

    current_test = None
    metrics = {}
    captured_score = False

    for line in lines:
        # Detect Start
        test_name = None
        if suite_type == "NPB":
            m = re_begin_npb.match(line)
            if m: test_name = m.group(1)
        elif suite_type == "UnixBench":
            m = re_begin_unix.match(line)
            if m: test_name = m.group(1)

        if test_name:
            current_test = test_name
            metrics = {}
            captured_score = False
            continue

        # Detect End
        if re_end.match(line):
            if current_test and 'score' in metrics:
                metrics['name'] = current_test
                results.append(metrics)
            current_test = None
            continue

        # Parse Metrics
        if current_test:
            if not captured_score:
                if suite_type == "NPB":
                    m = re_mops.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = "Mop/s"
                        captured_score = True
                elif suite_type == "UnixBench":
                    m = re_unix_count.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = m.group(3)
                        captured_score = True

            # PAPI Stats
            m_inst = re_papi_inst.match(line)
            if m_inst: metrics['instructions'] = int(m_inst.group(1))

            m_cyc = re_papi_cyc.match(line)
            if m_cyc: metrics['cycles'] = int(m_cyc.group(1))

            m_ipc = re_papi_ipc.match(line)
            if m_ipc: metrics['ipc'] = float(m_ipc.group(1))

    return results

# 3. Process Data
all_data = []
input_rows = cache_sweep[1:] if cache_sweep[0][0] == 'Bitstream' else cache_sweep

for row in input_rows:
    desc = row[1]
    npb_file = row[2]
    unix_file = row[3]

    # Create Short Config Tag
    config_tag = desc.replace("Instruction and Data caches = ", "IC/DC ").replace("Instruction and Data caches =", "IC/DC")

    # Parse NPB
    for res in parse_logs(os.path.join(NPB_DIR, npb_file), "NPB"):
        res['config'] = config_tag
        res['suite'] = "NPB"
        all_data.append(res)

    # Parse UnixBench
    for res in parse_logs(os.path.join(UNIX_DIR, unix_file), "UnixBench"):
        res['config'] = config_tag
        res['suite'] = "UnixBench"
        all_data.append(res)

# 4. Save to CSV
df = pd.DataFrame(all_data)
cols = ['config', 'suite', 'name', 'score', 'units', 'instructions', 'cycles', 'ipc']
df = df[[c for c in cols if c in df.columns]]

df.to_csv(OUTPUT_FILE, index=False)
print(f"Success! Data saved to: {OUTPUT_FILE}")
print(f"Total rows: {len(df)}")

#+end_src

#+RESULTS:
:results:
#+begin_exampleSuccess! Data saved to: /home/han4n/cva6_experiments/results/cache_sweep_benchmark_results.csv
Total rows: 80
#+end_example
:end:

** Associativity sweep

#+name: assoc_sweep
|-----------------------------------------------------------------------+---------------------+-------------------------+-------------------------------|
| *Bitstream*                                                             | *Characteristics*     | *NPB results file*        | *Unixbench results file*        |
|-----------------------------------------------------------------------+---------------------+-------------------------+-------------------------------|
| ariane_IC4k_1w_128L_DC4k_1w_128L_BTB16_BHT16_RAS2_ITLB4_DTLB4.bit.bin | Associativity 1 way | NPB_19700101_001807.txt | unixbench_19700101_003157.txt |
| ariane_IC4k_2w_128L_DC4k_2w_128L_BTB16_BHT16_RAS2_ITLB4_DTLB4.bit.bin | Associativity 2 way | NPB_19700101_000551.txt | unixbench_19700101_002129.txt |
| ariane_IC4k_8w_128L_DC4k_8w_128L_BTB16_BHT16_RAS2_ITLB4_DTLB4.bit.bin | Associativity 8 way | NPB_19700101_000259.txt | unixbench_19700101_002625.txt |
|-----------------------------------------------------------------------+---------------------+-------------------------+-------------------------------|

#+begin_src jupyter-python :var assoc_sweep=assoc_sweep  :results output drawer :exports results
import os
import re
import pandas as pd

# 1. Setup Paths
NPB_DIR = "/home/han4n/cva6_experiments/runs/NPB"
UNIX_DIR = "/home/han4n/cva6_experiments/runs/UnixBench"
OUTPUT_DIR = "/home/han4n/cva6_experiments/results"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "associativity_sweep_benchmark_results.csv")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# 2. Regex Patterns
re_begin_npb = re.compile(r">>>BEGIN_TEST\|NPB_(\w+)_")
re_begin_unix = re.compile(r">>>BEGIN_TEST\|(\w+)")
re_end = re.compile(r">>>END_TEST")

# Metrics
re_mops = re.compile(r"\s*Mop/s total\s*=\s*([\d\.]+)")
re_unix_count = re.compile(r"COUNT\|([\d\.]+)\|(\d+)\|(\w+)")
re_papi_inst = re.compile(r"\[PAPI\] Instructions:\s*(\d+)")
re_papi_cyc = re.compile(r"\[PAPI\] Cycles:\s*(\d+)")
re_papi_ipc = re.compile(r"\[PAPI\] IPC:\s*([\d\.]+)")

def parse_logs(file_path, suite_type):
    if not os.path.exists(file_path):
        print(f"Warning: File not found: {file_path}")
        return []

    results = []
    with open(file_path, 'r') as f:
        lines = f.readlines()

    current_test = None
    metrics = {}
    captured_score = False

    for line in lines:
        # Detect Start
        test_name = None
        if suite_type == "NPB":
            m = re_begin_npb.match(line)
            if m: test_name = m.group(1)
        elif suite_type == "UnixBench":
            m = re_begin_unix.match(line)
            if m: test_name = m.group(1)

        if test_name:
            current_test = test_name
            metrics = {}
            captured_score = False
            continue

        # Detect End
        if re_end.match(line):
            if current_test and 'score' in metrics:
                metrics['name'] = current_test
                results.append(metrics)
            current_test = None
            continue

        # Parse Metrics
        if current_test:
            if not captured_score:
                if suite_type == "NPB":
                    m = re_mops.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = "Mop/s"
                        captured_score = True
                elif suite_type == "UnixBench":
                    m = re_unix_count.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = m.group(3)
                        captured_score = True

            # PAPI Stats
            m_inst = re_papi_inst.match(line)
            if m_inst: metrics['instructions'] = int(m_inst.group(1))
            m_cyc = re_papi_cyc.match(line)
            if m_cyc: metrics['cycles'] = int(m_cyc.group(1))
            m_ipc = re_papi_ipc.match(line)
            if m_ipc: metrics['ipc'] = float(m_ipc.group(1))

    return results

# 3. Process Associativity Data
all_data = []

# Handle header row if present
input_rows = assoc_sweep
if len(input_rows) > 0 and ("Bitstream" in input_rows[0][0] or "*Bitstream*" in input_rows[0][0]):
    input_rows = input_rows[1:]

for row in input_rows:
    desc = row[1]
    npb_file = row[2]
    unix_file = row[3]

    # Clean up description for plotting (e.g., "Associativity 1 way" -> "1w")
    # This creates short tags: 1w, 2w, 8w
    config_tag = desc.lower().replace("associativity ", "").replace(" way", "w").strip()

    # Parse NPB
    for res in parse_logs(os.path.join(NPB_DIR, npb_file), "NPB"):
        res['config'] = config_tag
        res['suite'] = "NPB"
        all_data.append(res)

    # Parse UnixBench
    for res in parse_logs(os.path.join(UNIX_DIR, unix_file), "UnixBench"):
        res['config'] = config_tag
        res['suite'] = "UnixBench"
        all_data.append(res)

# 4. Save to CSV
if all_data:
    df = pd.DataFrame(all_data)
    cols = ['config', 'suite', 'name', 'score', 'units', 'instructions', 'cycles', 'ipc']
    df = df[[c for c in cols if c in df.columns]]

    df.to_csv(OUTPUT_FILE, index=False)
    print(f"Success! Associativity data saved to: {OUTPUT_FILE}")
    print(f"Total rows: {len(df)}")
else:
    print("Error: No data extracted.")
#+end_src

#+RESULTS:
:results:
#+begin_exampleSuccess! Associativity data saved to: /home/han4n/cva6_experiments/results/associativity_sweep_benchmark_results.csv
Total rows: 60
#+end_example
:end:


** BTB Size Sweep

#+name: btb_sweep
|-----------------------------------------------------------------------+-----------------+-------------------------+-------------------------------|
| *Bitstream*                                                             | *Characteristics* | *NPB results file*        | *Unixbench results file*        |
|-----------------------------------------------------------------------+-----------------+-------------------------+-------------------------------|
| ariane_IC4k_4w_128L_DC4k_4w_128L_BTB8_BHT16_RAS2_ITLB4_DTLB4.bit.bin  | BTB = 8         | NPB_19700101_000442.txt | unixbench_19700101_002126.txt |
| ariane_IC4k_4w_128L_DC4k_4w_128L_BTB32_BHT16_RAS2_ITLB4_DTLB4.bit.bin | BTB = 32        | NPB_19700101_000504.txt | unixbench_19700101_002518.txt |
| ariane_IC4k_4w_128L_DC4k_4w_128L_BTB64_BHT16_RAS2_ITLB4_DTLB4.bit.bin | BTB = 64        | NPB_19700101_000245.txt | unixbench_19700101_001827.txt |
|-----------------------------------------------------------------------+-----------------+-------------------------+-------------------------------|

#+begin_src jupyter-python :var btb_sweep=btb_sweep :results output drawer :exports results
import os
import re
import pandas as pd

# 1. Setup Paths
NPB_DIR = "/home/han4n/cva6_experiments/runs/NPB"
UNIX_DIR = "/home/han4n/cva6_experiments/runs/UnixBench"
OUTPUT_DIR = "/home/han4n/cva6_experiments/results"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "btb_sweep_benchmark_results.csv")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# 2. Regex Patterns
re_begin_npb = re.compile(r">>>BEGIN_TEST\|NPB_(\w+)_")
re_begin_unix = re.compile(r">>>BEGIN_TEST\|(\w+)")
re_end = re.compile(r">>>END_TEST")

# Metrics
re_mops = re.compile(r"\s*Mop/s total\s*=\s*([\d\.]+)")
re_unix_count = re.compile(r"COUNT\|([\d\.]+)\|(\d+)\|(\w+)")
re_papi_inst = re.compile(r"\[PAPI\] Instructions:\s*(\d+)")
re_papi_cyc = re.compile(r"\[PAPI\] Cycles:\s*(\d+)")
re_papi_ipc = re.compile(r"\[PAPI\] IPC:\s*([\d\.]+)")

def parse_logs(file_path, suite_type):
    if not os.path.exists(file_path):
        print(f"Warning: File not found: {file_path}")
        return []

    results = []
    with open(file_path, 'r') as f:
        lines = f.readlines()

    current_test = None
    metrics = {}
    captured_score = False

    for line in lines:
        test_name = None
        if suite_type == "NPB":
            m = re_begin_npb.match(line)
            if m: test_name = m.group(1)
        elif suite_type == "UnixBench":
            m = re_begin_unix.match(line)
            if m: test_name = m.group(1)

        if test_name:
            current_test = test_name
            metrics = {}
            captured_score = False
            continue

        if re_end.match(line):
            if current_test and 'score' in metrics:
                metrics['name'] = current_test
                results.append(metrics)
            current_test = None
            continue

        if current_test:
            if not captured_score:
                if suite_type == "NPB":
                    m = re_mops.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = "Mop/s"
                        captured_score = True
                elif suite_type == "UnixBench":
                    m = re_unix_count.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = m.group(3)
                        captured_score = True

            m_inst = re_papi_inst.match(line)
            if m_inst: metrics['instructions'] = int(m_inst.group(1))
            m_cyc = re_papi_cyc.match(line)
            if m_cyc: metrics['cycles'] = int(m_cyc.group(1))
            m_ipc = re_papi_ipc.match(line)
            if m_ipc: metrics['ipc'] = float(m_ipc.group(1))

    return results

# 3. Process BTB Data
all_data = []

input_rows = btb_sweep
if len(input_rows) > 0 and ("Bitstream" in input_rows[0][0] or "*Bitstream*" in input_rows[0][0]):
    input_rows = input_rows[1:]

for row in input_rows:
    desc = row[1]
    npb_file = row[2]
    unix_file = row[3]

    # Create Short Config Tag (e.g., "BTB = 8" -> "BTB8")
    config_tag = desc.replace(" = ", "").strip()

    # Parse NPB
    for res in parse_logs(os.path.join(NPB_DIR, npb_file), "NPB"):
        res['config'] = config_tag
        res['suite'] = "NPB"
        all_data.append(res)

    # Parse UnixBench
    for res in parse_logs(os.path.join(UNIX_DIR, unix_file), "UnixBench"):
        res['config'] = config_tag
        res['suite'] = "UnixBench"
        all_data.append(res)

# 4. Save to CSV
if all_data:
    df = pd.DataFrame(all_data)
    cols = ['config', 'suite', 'name', 'score', 'units', 'instructions', 'cycles', 'ipc']
    df = df[[c for c in cols if c in df.columns]]

    df.to_csv(OUTPUT_FILE, index=False)
    print(f"Success! BTB data saved to: {OUTPUT_FILE}")
    print(f"Total rows: {len(df)}")
else:
    print("Error: No data extracted.")
#+end_src

#+RESULTS:
:results:
#+begin_exampleSuccess! BTB data saved to: /home/han4n/cva6_experiments/results/btb_sweep_benchmark_results.csv
Total rows: 60
#+end_example
:end:


** BHT Size Sweep

#+name: bht_sweep
|------------------------------------------------------------------------+-----------------+-------------------------+-------------------------------|
| *Bitstream*                                                              | *Characteristics* | *NPB results file*        | *Unixbench results file*        |
|------------------------------------------------------------------------+-----------------+-------------------------+-------------------------------|
| ariane_IC4k_4w_128L_DC4k_4w_128L_BTB16_BHT32_RAS2_ITLB4_DTLB4.bit.bin  | BHT = 32        | NPB_19700101_002319.txt | unixbench_19700101_003551.txt |
| ariane_IC4k_4w_128L_DC4k_4w_128L_BTB16_BHT64_RAS2_ITLB4_DTLB4.bit.bin  | BHT = 64        | NPB_19700101_000352.txt | unixbench_19700101_004015.txt |
| ariane_IC4k_4w_128L_DC4k_4w_128L_BTB16_BHT128_RAS2_ITLB4_DTLB4.bit.bin | BHT = 128       | NPB_19700101_001913.txt | unixbench_19700101_003121.txt |
|------------------------------------------------------------------------+-----------------+-------------------------+-------------------------------|

#+begin_src jupyter-python :var bht_sweep=bht_sweep :results output drawer :exports results
import os
import re
import pandas as pd

# 1. Setup Paths
NPB_DIR = "/home/han4n/cva6_experiments/runs/NPB"
UNIX_DIR = "/home/han4n/cva6_experiments/runs/UnixBench"
OUTPUT_DIR = "/home/han4n/cva6_experiments/results"
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "bht_sweep_benchmark_results.csv")

os.makedirs(OUTPUT_DIR, exist_ok=True)

# 2. Regex Patterns
re_begin_npb = re.compile(r">>>BEGIN_TEST\|NPB_(\w+)_")
re_begin_unix = re.compile(r">>>BEGIN_TEST\|(\w+)")
re_end = re.compile(r">>>END_TEST")

# Metrics
re_mops = re.compile(r"\s*Mop/s total\s*=\s*([\d\.]+)")
re_unix_count = re.compile(r"COUNT\|([\d\.]+)\|(\d+)\|(\w+)")
re_papi_inst = re.compile(r"\[PAPI\] Instructions:\s*(\d+)")
re_papi_cyc = re.compile(r"\[PAPI\] Cycles:\s*(\d+)")
re_papi_ipc = re.compile(r"\[PAPI\] IPC:\s*([\d\.]+)")

def parse_logs(file_path, suite_type):
    if not os.path.exists(file_path):
        print(f"Warning: File not found: {file_path}")
        return []

    results = []
    with open(file_path, 'r') as f:
        lines = f.readlines()

    current_test = None
    metrics = {}
    captured_score = False

    for line in lines:
        test_name = None
        if suite_type == "NPB":
            m = re_begin_npb.match(line)
            if m: test_name = m.group(1)
        elif suite_type == "UnixBench":
            m = re_begin_unix.match(line)
            if m: test_name = m.group(1)

        if test_name:
            current_test = test_name
            metrics = {}
            captured_score = False
            continue

        if re_end.match(line):
            if current_test and 'score' in metrics:
                metrics['name'] = current_test
                results.append(metrics)
            current_test = None
            continue

        if current_test:
            if not captured_score:
                if suite_type == "NPB":
                    m = re_mops.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = "Mop/s"
                        captured_score = True
                elif suite_type == "UnixBench":
                    m = re_unix_count.match(line)
                    if m:
                        metrics['score'] = float(m.group(1))
                        metrics['units'] = m.group(3)
                        captured_score = True

            m_inst = re_papi_inst.match(line)
            if m_inst: metrics['instructions'] = int(m_inst.group(1))
            m_cyc = re_papi_cyc.match(line)
            if m_cyc: metrics['cycles'] = int(m_cyc.group(1))
            m_ipc = re_papi_ipc.match(line)
            if m_ipc: metrics['ipc'] = float(m_ipc.group(1))

    return results

# 3. Process BHT Data
all_data = []

input_rows = bht_sweep
if len(input_rows) > 0 and ("Bitstream" in input_rows[0][0] or "*Bitstream*" in input_rows[0][0]):
    input_rows = input_rows[1:]

for row in input_rows:
    desc = row[1]
    npb_file = row[2]
    unix_file = row[3]

    # Create Short Config Tag (e.g., "BHT = 32" -> "BHT32")
    config_tag = desc.replace(" = ", "").replace("Entries", "").strip()

    # Parse NPB
    for res in parse_logs(os.path.join(NPB_DIR, npb_file), "NPB"):
        res['config'] = config_tag
        res['suite'] = "NPB"
        all_data.append(res)

    # Parse UnixBench
    for res in parse_logs(os.path.join(UNIX_DIR, unix_file), "UnixBench"):
        res['config'] = config_tag
        res['suite'] = "UnixBench"
        all_data.append(res)

# 4. Save to CSV
if all_data:
    df = pd.DataFrame(all_data)
    cols = ['config', 'suite', 'name', 'score', 'units', 'instructions', 'cycles', 'ipc']
    df = df[[c for c in cols if c in df.columns]]

    df.to_csv(OUTPUT_FILE, index=False)
    print(f"Success! BHT data saved to: {OUTPUT_FILE}")
    print(f"Total rows: {len(df)}")
else:
    print("Error: No data extracted.")
#+end_src

#+RESULTS:
:results:
#+begin_exampleSuccess! BHT data saved to: /home/han4n/cva6_experiments/results/bht_sweep_benchmark_results.csv
Total rows: 60
#+end_example
:end:
