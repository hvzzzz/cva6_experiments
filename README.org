#+TITLE: Experiment Log
Each bitstream will produce 4 results: UnixBench, NPB and also performance counters will be measured in each case:
* Baseline bitstream
:PROPERTIES:
:ID:       d2932afd-bb0d-4d36-9f4f-4ee70c8ff9ee
:END:
This bitstream is the bitstream at the end of the [[https://pages.saclay.inria.fr/nicolas.derumigny/COMPAS-Tutorial.html][COMPAS 2025 tutorial]] with performance counters enabled at: ~/cva6/core/include/cv64a6_imafdch_sv39_config_pkg.sv

#+begin_src bash
   localparam CVA6ConfigPerfCounterEn = 1;
#+end_src

** Project Summary
*** Implementation Utilization
#+begin_src jupyter-python :exports results
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ==========================================
# 1. CONFIGURATION
# ==========================================
REPORT_PATH = "./bitstreams/SoC_wrapper_utilization_placed_base.rpt"

# ==========================================
# 2. PARSER LOGIC
# ==========================================

def parse_vivado_report_full(content):
    """
    Scans the report and extracts specific keys into a dictionary.
    """
    data = {}

    # Map: Label for Graph -> Text in Report
    target_map = {
        'LUT': 'Slice LUTs',
        'LUTRAM': 'LUT as Memory',
        'FF': 'Slice Registers',
        'BRAM': 'Block RAM Tile',
        'DSP': 'DSPs',
        'I/O': 'Bonded IOB',
        'BUFG': 'BUFGCTRL',
        'MMCM': 'MMCME2_ADV'
    }

    lines = content.split('\n')

    for line in lines:
        line = line.strip()
        if line.startswith('|'):
            parts = [x.strip() for x in line.split('|') if x.strip()]
            if len(parts) >= 2:
                row_name = parts[0].replace('*', '')
                row_val_str = parts[1]

                for user_label, vivado_key in target_map.items():
                    if row_name == vivado_key:
                        try:
                            data[user_label] = float(row_val_str)
                        except ValueError:
                            pass

    return data

# ==========================================
# 3. VISUALIZATION
# ==========================================

# 1. Load Data
if os.path.exists(REPORT_PATH):
    print(f"✅ Reading report from: {REPORT_PATH}")
    with open(REPORT_PATH, 'r') as f:
        content = f.read()
else:
    print(f"⚠️ File not found at {REPORT_PATH}. Using Sample Data.")
    content = SAMPLE_REPORT_DATA

# 2. Parse and Create DataFrame
metrics = parse_vivado_report_full(content)

# Specific order requested
order = ['MMCM', 'BUFG', 'I/O', 'DSP', 'BRAM', 'FF', 'LUTRAM', 'LUT']

df = pd.DataFrame([
    {'Resource': k, 'Count': metrics.get(k, 0)}
    for k in order
])

# 3. Display Table (The requested change)
# print("\n--- Absolute Resource Usage ---")
# Using Pandas styling for a cleaner look in Jupyter
# display(df.style.background_gradient(subset=['Count'], cmap='Blues'))

# 4. Plot Graph
sns.set_theme(style="whitegrid")
plt.figure(figsize=(12, 6))

ax = sns.barplot(
    data=df,
    x='Resource',
    y='Count',
    hue='Resource',
    palette='viridis'
)

ax.set_yscale("log") # Log Scale
plt.title('CVA6 FPGA Resource Usage (Absolute Count)', fontsize=16)
plt.ylabel('Count (Log Scale)', fontsize=12)
plt.xlabel('')

# Add labels
for i, p in enumerate(ax.patches):
    height = p.get_height()
    if height > 0:
        val_text = f"{int(height)}" if height.is_integer() else f"{height}"
        ax.text(p.get_x() + p.get_width() / 2., height * 1.1,
                val_text,
                ha="center", va="bottom", fontsize=11, fontweight='bold')

plt.tight_layout()
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: ✅ Reading report from: ./bitstreams/SoC_wrapper_utilization_placed_base.rpt
[[file:./.ob-jupyter/80f69f1fc27fac1f504bac7de189b99ab81f522b.png]]
:END:
*** Implementation Timing
#+begin_src jupyter-python :results output drawer :exports results
import pandas as pd
import os

# ==========================================
# 1. CONFIGURATION
# ==========================================
TIMING_REPORT_PATH = "./bitstreams/SoC_wrapper_timing_summary_routed_base.rpt"

# ==========================================
# 2. PARSER LOGIC
# ==========================================

def parse_timing_report(content):
    data = {}
    lines = content.split('\n')

    # State flags
    header_found = False

    for line in lines:
        # Detect the table header row
        if "WNS(ns)" in line and "WHS(ns)" in line:
            header_found = True
            continue

        # Parse the data row (usually the first non-empty line after dashes)
        if header_found and line.strip() and not line.strip().startswith('-'):
            parts = line.split()

            # Based on standard Vivado report columns:
            # Col 0: WNS (Setup)
            # Col 4: WHS (Hold)
            try:
                wns = float(parts[0])
                whs = float(parts[4])

                data['Metric'] = ['WNS (Setup)', 'WHS (Hold)']
                data['Value (ns)'] = [wns, whs]

            except (ValueError, IndexError):
                pass

            break # We only need the summary row

    return pd.DataFrame(data)

# ==========================================
# 3. EXECUTION
# ==========================================

# 1. Load
if os.path.exists(TIMING_REPORT_PATH):
    print(f"✅ Reading timing report from: {TIMING_REPORT_PATH}")
    with open(TIMING_REPORT_PATH, 'r') as f:
        content = f.read()
else:
    print(f"⚠️ File not found at {TIMING_REPORT_PATH}. Using Sample Data.")
    content = SAMPLE_TIMING_DATA

# 2. Parse
df_timing = parse_timing_report(content)

# 3. Display
print("\n--- Timing Summary ---")

# Styling: Red if negative (Time Violation), Green if positive (Safe)
def style_slack(val):
    color = 'red' if val < 0 else 'green'
    weight = 'bold'
    return f'color: {color}; font-weight: {weight}'

# Display with 3 decimal places
format_dict = {'Value (ns)': '{:.3f}'}

display(df_timing.style.format(format_dict).map(style_slack, subset=['Value (ns)']).hide(axis="index"))
#+end_src

#+RESULTS:
:results:
#+begin_example✅ Reading timing report from: ./bitstreams/SoC_wrapper_timing_summary_routed_base.rpt

--- Timing Summary ---
#+end_example
| Metric      | Value (ns) |
|-------------+------------|
| WNS (Setup) | 1.218      |
| WHS (Hold)  | 0.051      |
:end:


*** File locations
**** Bitstream
[[./bitstreams/baseline.bit]]
**** Implementation utilization report
[[./bitstreams/SoC_wrapper_utilization_placed_base.rpt]]
**** Implementation timing report
[[./bitstreams/SoC_wrapper_timing_summary_routed_base.rpt]]
**** Implementation power report
[[./bitstreams/SoC_wrapper_power_routed_base.rpt]]
** UnixBench
Experiment metadata
[[./runs/UnixBench/unixbench_19700101_064547_base.txt]]

#+begin_src jupyter-python :exports results
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ==========================================
# 1. CONFIGURATION
# ==========================================

# Path to your specific log file
LOG_FILE_PATH = "./runs/UnixBench/unixbench_19700101_064547_base.txt"


# ==========================================
# 2. DATA LOADING & PARSING
# ==========================================

def load_data(path):
    if os.path.exists(path):
        print(f"✅ Loading data from: {path}")
        with open(path, "r") as f:
            return f.read()
    else:
        print(f"⚠️ File not found at {path}. Using Sample Data for demonstration.")
        return SAMPLE_DATA

def parse_log_data(raw_text):
    data = []
    current_test = {}
    in_test = False

    lines = raw_text.split('\n')

    for line in lines:
        line = line.strip()

        # Detect Start of Test
        if ">>>BEGIN_TEST" in line:
            test_name = line.split("|")[1]
            current_test = {
                'Test': test_name,
                'Instructions': 0,
                'Cycles': 0,
                'IPC': 0.0,
                'Score': 0.0,
                'Unit': 'N/A',
                'Category': 'Other'
            }

            # Categorize for better plotting
            if 'Arith' in test_name or 'Dhrystone' in test_name or 'Register' in test_name:
                current_test['Category'] = 'Integer/ALU'
            elif 'Whetstone' in test_name or 'Double' in test_name:
                current_test['Category'] = 'Float/FPU'
            elif 'Syscall' in test_name or 'Context' in test_name or 'Pipe' in test_name:
                current_test['Category'] = 'System/OS'

            in_test = True
            continue

        if in_test:
            # Parse PAPI Headers
            if "[PAPI] Instructions:" in line:
                current_test['Instructions'] = int(line.split(":")[1].strip())
            elif "[PAPI] Cycles:" in line:
                current_test['Cycles'] = int(line.split(":")[1].strip())
            elif "[PAPI] IPC:" in line:
                current_test['IPC'] = float(line.split(":")[1].strip())

            # Parse UnixBench Scores (COUNT|score|...|unit)
            elif line.startswith("COUNT|"):
                parts = line.split("|")
                try:
                    current_test['Score'] = float(parts[1])
                    current_test['Unit'] = parts[3].strip()
                except IndexError:
                    pass

            # Parse Whetstone specific format (MWIPS at end of line)
            elif "MWIPS" in line and "COUNT" not in line:
                 parts = line.split()
                 if len(parts) >= 2:
                     try:
                         # Usually formatted like: MWIPS   19.449   9.769
                         current_test['Score'] = float(parts[1])
                         current_test['Unit'] = 'MWIPS'
                     except ValueError:
                         pass

            # Detect End
            elif ">>>END_TEST" in line:
                # Cleanup Label name
                current_test['Label'] = current_test['Test'].replace('Arithmetic_', '').replace('Syscall_', '').replace('UB_', '')
                data.append(current_test)
                in_test = False

    return pd.DataFrame(data)

# Load and Parse
log_content = load_data(LOG_FILE_PATH)
df = parse_log_data(log_content)

# ==========================================
# 3. VISUALIZATION
# ==========================================

if not df.empty:
    # Set Style
    sns.set_theme(style="whitegrid")
    plt.rcParams.update({'figure.figsize': (14, 10), 'font.size': 12})

    fig, axes = plt.subplots(2, 1, height_ratios=[1, 1.2])

    # --- PLOT 1: Efficiency (IPC) ---
    sns.barplot(
        data=df.sort_values('IPC', ascending=False),
        x='IPC',
        y='Label',
        hue='Category',
        dodge=False,
        palette='viridis',
        ax=axes[0]
    )
    axes[0].set_title('CVA6 Architectural Efficiency (Instructions Per Cycle)', fontsize=16, pad=15)
    axes[0].set_xlabel('IPC (Higher is Better)')
    axes[0].set_ylabel('')
    axes[0].set_xlim(0, 0.7)
    axes[0].legend(title='Workload Type', loc='lower right')

    for container in axes[0].containers:
        axes[0].bar_label(container, fmt='%.2f', padding=3)

    # --- PLOT 2: Benchmark Scores ---
    sns.barplot(
        data=df.sort_values('Score', ascending=False),
        x='Score',
        y='Label',
        hue='Unit',
        dodge=False,
        palette='magma',
        ax=axes[1]
    )
    axes[1].set_xscale('log')
    axes[1].set_title('UnixBench Scores (Log Scale)', fontsize=16, pad=15)
    axes[1].set_xlabel('Score (Log Scale) - Note Different Units!')
    axes[1].set_ylabel('')
    axes[1].legend(title='Metric Unit', loc='lower right')

    for container in axes[1].containers:
        axes[1].bar_label(container, fmt='%.1e', padding=3)

    plt.tight_layout()
    plt.show()

    # --- DATA TABLE ---
    print("\n=== Parsed Benchmark Data ===")
    display_cols = ['Test', 'Category', 'IPC', 'Instructions', 'Score', 'Unit']
    # Use Pandas styling for Jupyter
    display(df[display_cols].style.background_gradient(subset=['IPC'], cmap='RdYlGn'))

else:
    print("❌ No valid test data found in the file.")
#+end_src

#+RESULTS:
:RESULTS:
: ✅ Loading data from: ./runs/UnixBench/unixbench_19700101_064547_base.txt
[[file:./.ob-jupyter/f8cc59f5363990aa4ce071645d4d01577b5be820.png]]
:
: === Parsed Benchmark Data ===
|    | Test                | Category    | IPC      | Instructions | Score           | Unit  |
|----+---------------------+-------------+----------+--------------+-----------------+-------|
| 0  | Arithmetic_Overhead | Integer/ALU | 0.388300 | 96160360     | 22607716.000000 | lps   |
| 1  | Arithmetic_Register | Integer/ALU | 0.568500 | 140705146    | 658341.000000   | lps   |
| 2  | Arithmetic_Short    | Integer/ALU | 0.569000 | 140859170    | 659170.000000   | lps   |
| 3  | Arithmetic_Int      | Integer/ALU | 0.568700 | 140769091    | 658720.000000   | lps   |
| 4  | Dhrystone           | Integer/ALU | 0.447600 | 81389671     | 259837.000000   | lps   |
| 5  | Arithmetic_Double   | Integer/ALU | 0.568800 | 140675372    | 658256.000000   | lps   |
| 6  | Whetstone           | Float/FPU   | 0.410800 | 103499392    | 19.449000       | MWIPS |
| 7  | Syscall_Mix         | System/OS   | 0.174900 | 43254593     | 22651.000000    | lps   |
| 8  | Syscall_GetPID      | System/OS   | 0.225900 | 55509267     | 205011.000000   | lps   |
| 9  | Syscall_Exec        | System/OS   | 0.119100 | 2620492      | 20.000000       | lps   |
| 10 | Pipe_Throughput     | System/OS   | 0.140400 | 34742090     | 8840.000000     | lps   |
| 11 | Context_Switching   | System/OS   | 0.116900 | 14197502     | 856.000000      | lps   |
:END:

** NAS Parallel Benchmark
Experiment metadata
[[./runs/NPB/NPB_19700101_081823_base.txt]]

#+begin_src jupyter-python :exports results
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# ==========================================
# 1. CONFIGURATION
# ==========================================

LOG_FILE_PATH = "runs/NPB/NPB_19700101_081823_base.txt"

# ==========================================
# 2. PARSING LOGIC
# ==========================================

def load_data(path):
    if os.path.exists(path):
        print(f"✅ Loading data from: {path}")
        with open(path, "r", encoding='utf-8', errors='ignore') as f:
            return f.read()
    else:
        print(f"⚠️ File not found at {path}. Using Sample Data.")

def parse_npb_log(raw_text):
    data = []
    current_test = {}
    in_test = False

    lines = raw_text.split('\n')

    for line in lines:
        line = line.strip()

        if ">>>BEGIN_TEST" in line:
            test_name = line.split("|")[1]
            current_test = {
                'Test': test_name,
                'Label': test_name.replace('NPB_', '').replace('_S', ''), # Clean name
                'Instructions': 0,
                'Cycles': 0,
                'IPC': 0.0,
                'Score': 0.0,
                'Category': 'Other'
            }

            # Categorize based on workload type
            label = current_test['Label']
            if label == 'EP':
                current_test['Category'] = 'Compute Bound'
            elif label in ['IS', 'CG', 'MG']:
                current_test['Category'] = 'Memory Bound'
            elif label in ['FT', 'LU', 'SP', 'BT']:
                current_test['Category'] = 'Mixed / Streaming'

            in_test = True
            continue

        if in_test:
            # Parse PAPI
            if "[PAPI] Instructions:" in line:
                current_test['Instructions'] = int(line.split(":")[1].strip())
            elif "[PAPI] Cycles:" in line:
                current_test['Cycles'] = int(line.split(":")[1].strip())
            elif "[PAPI] IPC:" in line:
                current_test['IPC'] = float(line.split(":")[1].strip())

            # Parse Score (Mop/s)
            elif "Mop/s total" in line:
                try:
                    # Format: Mop/s total      =                    2.15
                    parts = line.split("=")
                    current_test['Score'] = float(parts[1].strip())
                except:
                    pass

            # Detect End
            elif ">>>END_TEST" in line:
                # Handle IS crash if present
                if current_test['IPC'] == 0 and current_test['Cycles'] == 0:
                    current_test['Label'] += " (Failed)"

                data.append(current_test)
                in_test = False

    return pd.DataFrame(data)

# Load
log_content = load_data(LOG_FILE_PATH)
df = parse_npb_log(log_content)

# Filter out failed tests (like IS if it segfaulted)
df = df[df['Cycles'] > 0]

# ==========================================
# 3. VISUALIZATION
# ==========================================

if not df.empty:
    sns.set_theme(style="whitegrid")
    plt.rcParams.update({'figure.figsize': (14, 10), 'font.size': 12})

    fig, axes = plt.subplots(2, 1, height_ratios=[1, 1])

    # --- PLOT 1: Efficiency (IPC) ---
    sns.barplot(
        data=df.sort_values('IPC', ascending=False),
        x='IPC',
        y='Label',
        hue='Category',
        dodge=False,
        palette='viridis',
        ax=axes[0]
    )
    axes[0].set_title('CVA6 Performance on NPB Class S (IPC)', fontsize=16, pad=15)
    axes[0].set_xlabel('Instructions Per Cycle (Higher is Better)')
    axes[0].set_ylabel('')
    axes[0].legend(title='Workload Type', loc='lower right')
    axes[0].set_xlim(0, 0.4) # Adjust limit based on max value (EP is ~0.3)

    for container in axes[0].containers:
        axes[0].bar_label(container, fmt='%.3f', padding=3)

    # --- PLOT 2: MFLOPS Scores ---
    sns.barplot(
        data=df.sort_values('Score', ascending=False),
        x='Score',
        y='Label',
        hue='Category',
        dodge=False,
        palette='magma',
        ax=axes[1]
    )
    axes[1].set_title('Absolute Performance (Mop/s)', fontsize=16, pad=15)
    axes[1].set_xlabel('Million Operations Per Second')
    axes[1].set_ylabel('')
    axes[1].legend(title='Workload Type', loc='lower right')

    for container in axes[1].containers:
        axes[1].bar_label(container, fmt='%.2f', padding=3)

    plt.tight_layout()
    plt.show()

    # --- DATA TABLE ---
    print("\n=== NAS Parallel Benchmark Results ===")
    display_cols = ['Label', 'Category', 'IPC', 'Instructions', 'Cycles', 'Score']
    display(df[display_cols].style.background_gradient(subset=['IPC'], cmap='RdYlGn'))

else:
    print("❌ No valid data found. Check the log file path.")
#+end_src

#+RESULTS:
:RESULTS:
: ✅ Loading data from: runs/NPB/NPB_19700101_081823_base.txt
[[file:./.ob-jupyter/d551c1b9487672ade4da408129495ec5f7d8f13e.png]]
:
: === NAS Parallel Benchmark Results ===
|   | Label | Category          | IPC      | Instructions | Cycles     | Score    |
|---+-------+-------------------+----------+--------------+------------+----------|
| 0 | EP    | Compute Bound     | 0.296600 | 2091610366   | 7052575499 | 0.120000 |
| 2 | CG    | Memory Bound      | 0.167400 | 355809507    | 2125967585 | 0.780000 |
| 3 | MG    | Memory Bound      | 0.193000 | 33938262     | 175850655  | 1.070000 |
| 4 | FT    | Mixed / Streaming | 0.159500 | 586546054    | 3678139437 | 1.190000 |
| 5 | BT    | Mixed / Streaming | 0.209400 | 549647248    | 2625329283 | 2.150000 |
| 6 | SP    | Mixed / Streaming | 0.181100 | 357286022    | 1972775720 | 1.210000 |
| 7 | LU    | Mixed / Streaming | 0.230000 | 200279517    | 870961646  | 2.900000 |
:END:

#+begin_src jupyter-python :results output drawer
print('sfasdf')
#+end_src




#+begin_src emacs-lisp
(setq org-babel-min-lines-for-block-output 0)
#+end_src

#+RESULTS:
#+begin_example
0
#+end_example
